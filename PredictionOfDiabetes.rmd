---
title: "Project_R"
author: "Team_3"
date: "June 15, 2023"
output: word_document
---
```{r}
######a) Load the dataset in R Studio. Examine the first few rows of data using R. Explain your 
findings.

#setting library
#setwd("~/Desktop/SIP690/Project_R")
#load data
patients <- read.csv("patients.csv")
#head 
head(patients, n=30)
```

```{r}
#######b) Provide summary statistics. Calculate the mean, median, standard deviation, and quartiles for 
each independent variable. Explain your results. 

#Summary statistics
summary(patients[, c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "Pedigree", "Age")]) 

####Calculate mean, median, standard deviation, and quartiles for each variable 
summary_stats <- data.frame( 
  Variable = names(patients), 
  Mean = colMeans(patients), 
  Median = apply(patients, 2, median), 
  Standard_Deviation = apply(patients, 2, sd), 
  Q1 = apply(patients, 2, quantile, 0.25), 
  Q3 = apply(patients, 2, quantile, 0.75) 
) 
 
print(summary_stats) 
```

```{r}
#######c) Using the ggplot2 library, create any five visualizations. Explain your reasoning for selecting 
those visualizations. Explain the output of each visualization. What are the insights your 
visualizations reveal about the dataset.

###1.distribution of glucose levels among the patients

library(ggplot2) 
 
ggplot(data = patients, aes(x = Glucose)) + 
geom_histogram(binwidth = 10, fill = "#7463ac", color = "white") + 
  labs(title = "Distribution of Glucose levels", 
   x = "Glucose Levels", 
 y = "Frequency") 

###2.Distribution of BMI between patients with and without a diabetes diagnosis

ggplot(data = patients, aes(x = as.factor(Diagnosis), y = BMI, fill = as.factor(Diagnosis))) + 
  geom_boxplot() + 
  labs(title = "Boxplot of BMI and Diabetes Diagnosis", 
       x = "Diabetes Diagnosis", 
       y = "BMI") 
###3.relationship between age and blood pressure.

ggplot(data = patients, aes(x = Age, y = BloodPressure, color = as.factor(Diagnosis))) + 
  geom_point() + 
  labs(title = "Scatterplot of Age and Blood Pressure", 
       x = "Age", 
       y = "Blood Pressure") 
###4.Relationship between BMI and Age

ggplot(patients, aes(x = BMI, y = Age, color = BMI)) +
  geom_point() +
  labs(x = "BMI", y = "Age") +
  ggtitle("Scatter Plot of BMI vs. Age")

###5.Count or proportion of patients with and without a diabetes diagnosis

ggplot(data = patients, aes(x = as.factor(Diagnosis), fill = as.factor(Diagnosis))) + 
  geom_bar() + 
  labs(title = "Bar Plot of Diabetes Diagnosis", 
       x = "Diabetes Diagnosis", 
       y = "Count") 

```
```{r}
#######d) Find missing values for each independent variable and fill them with median values. The 
missing values for independent variables in the dataset are coded 0

#visualization of missing values
patients$Glucose[patients$Glucose %in% 0] <- NA
patients$BloodPressure[patients$BloodPressure %in% 0] <- NA
patients$SkinThickness[patients$SkinThickness %in% 0] <- NA
patients$Insulin[patients$Insulin %in% 0] <- NA
patients$BMI[patients$BMI %in% 0] <- NA
patients$Pregnancies[patients$Pregnancies %in% 0] <- NA
patients$Age[patients$Age %in% 0] <- NA
patients$Pedigree[patients$Pedigree %in% 0] <- NA
mis_1 <- data.frame(val=apply(patients[-9],2,function(x){round(sum(is.na(x))/nrow(patients),2)*100}),names = names(apply(patients[-9],2,function(x){sum(is.na(x))})))
mis_1 <- mis_1[order(mis_1$val),]
mis_1$names <- factor(mis_1$names,levels = mis_1$names)
ggplot(mis_1)+ 
  geom_bar(mapping = aes(x = names, y = val),
           stat = "identity", fill = '#7463AC')+
  geom_text(aes(x = names, y = val,label=paste0(val,"%")),vjust=-0.65)+ylab("Missing (%)")+xlab("Features")

## Identify missing values (coded as 0) in each independent variable (excluding "Diagnosis") 
missing_values_1 <- sapply(patients[, -which(names(patients) == "Diagnosis")], function(x) sum(x == 0)) 
print(missing_values_1)
```


```{r}
#d)replacing missing values
                           
patients_1 <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "Pedigree", "Age")
```


```{r}
# d)
#Iterate over each column (excluding the "Diagnosis" column)
for (col in names(patients_1)[-which(names(patients) == "Diagnosis")]) {
  # Find the indices of missing values (coded as 0)
  missing_indices <- which(patients[[col]] == 0)
  
  # Calculate the median excluding the missing values
  median_value <- median(patients[[col]][patients[[col]] != 0], na.rm = TRUE)
  
  # Replace the missing values with the median
  patients[missing_indices, col] <- median_value
}
patients
```
```{r}
#d)
# Now calculate the median for each independent variable.

median_values <- sapply(patients[, patients_1], median, na.rm = TRUE)
print(median_values)
# After calculating the median values, now fill the missing values with median values.
for (attr in patients_1) {

  patients[is.na(patients[[attr]]), attr] <- median_values[attr]

}
# Let's check whether the NA values have been replaced by median.
head(patients)
```


```{r}
#d)
#plot of  missing values after replacing it with median (check)
patients$Glucose[patients$Glucose %in% 0] <- NA
patients$BloodPressure[patients$BloodPressure %in% 0] <- NA
patients$SkinThickness[patients$SkinThickness %in% 0] <- NA
patients$Insulin[patients$Insulin %in% 0] <- NA
patients$BMI[patients$BMI %in% 0] <- NA
patients$Pregnancies[patients$Pregnancies %in% 0] <- NA
patients$Age[patients$Age %in% 0] <- NA
patients$Pedigree[patients$Pedigree %in% 0] <- NA
mis_2 <- mis_1
mis_2 <- data.frame(val=apply(patients[-9],2,function(x){round(sum(is.na(x))/nrow(patients),2)*100}),names = names(apply(patients[-9],2,function(x){sum(is.na(x))})))
mis_2 <- mis_2[order(mis_2$val),]
mis_2$names <- factor(mis_2$names,levels = mis_2$names)
ggplot(mis_2)+ 
  geom_bar(mapping = aes(x = names, y = val),
           stat = "identity", fill = '#7463AC')+
  geom_text(aes(x = names, y = val,label=paste0(val,"%")),vjust=-0.65)+ylab("Missing (%)")+xlab("Features")

```


```{r}
#######e) Find outliers for each independent variable using the IQR rule. 
                           
#Define a function to detect outliers using the IQR rule 
detect_outliers <- function(x) { 
  q1 <- quantile(x, 0.25) 
  q3 <- quantile(x, 0.75) 
  iqr <- q3 - q1 
  lower_bound <- q1 - 1.5 * iqr 
  upper_bound <- q3 + 1.5 * iqr 
  outliers <- x[x < lower_bound | x > upper_bound] 
  return(outliers) 
} 
 
# Find outliers for each independent variable 
outliers <- lapply(patients[, -which(names(patients) == "Diagnosis")], detect_outliers) 
 
# Print the outliers for each variable 
for (i in seq_along(outliers)) { 
  variable <- names(patients[, -which(names(patients) == "Diagnosis")])[i] 
  cat("Outliers for", variable, ":") 
  if (length(outliers[[i]]) == 0) { 
    cat(" None\n") 
  } else { 
    cat("\n") 
    print(outliers[[i]]) 
  } 
} 
```

```{r}
patients_2 <- patients
summary(patients_2)
```

```{r}
#######f) Approach for replacing outlier with lower and upper bound
                           
for (i in seq_along(outliers)) { 
  variable <- names(patients_2[, -which(names(patients_2) == "Diagnosis")])[i] 
   
  if (variable != "Diagnosis" && length(outliers[[i]]) > 0) { 
    q1 <- quantile(patients_2[[variable]], 0.25) 
    q3 <- quantile(patients_2[[variable]], 0.75) 
    iqr <- q3 - q1 
    lower_bound <- q1 - 1.5 * iqr 
    upper_bound <- q3 + 1.5 * iqr 
    outliers_indices <- which(patients_2[[variable]] %in% outliers[[i]]) 
     
for (index in outliers_indices) { 
      if (patients_2[[variable]][index] < lower_bound || patients_2[[variable]][index] > upper_bound) { 
        if (patients_2[[variable]][index] < lower_bound) { 
          patients_2[[variable]][index] <- max(patients_2[[variable]][patients_2[[variable]] >= lower_bound]) 
        } else { 
          patients_2[[variable]][index] <- min(patients_2[[variable]][patients_2[[variable]] <= upper_bound]) 
        } 
      } 
    } 
  } 
} 
summary(patients_2)
```

```{r}
#######g) Best performing variables 
                           
#install.packages("corrplot")
library(corrplot)
#correlation_matrix <- cor(patients_2)

# Select the variables of interest
variables_of_interest <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI", "Pedigree", "Age")

# Subset the dataset to include only the selected variables
subset_data <- patients_2[, variables_of_interest]

# Calculate the correlation matrix
correlation_matrix <- cor(subset_data)
correlation_matrix

```

```{r}
#g)
# Install and load the ggplot2 package
#install.packages("ggplot2")
library(ggplot2)
library(corrplot)

# Create the correlation plot using a heatmap

#corrplot(correlation_matrix, method = "color")
#corMat = cor (patients_2[, -9])
corMat = cor (patients_2)
diag (corMat) = 0 #Remove self correlations
corrplot.mixed(corMat,tl.pos = "lt")
```
```{r}
#######h) Standardize your features to Gaussian distribution. Explain why it would be a good idea to 
standardize the features to Gaussian distribution.

# Standardize the selected variables to Gaussian distribution 
standardized_data <- scale(patients_2[, -9])
 
# Convert the standardized data back to a data frame 
standardized_data <- as.data.frame(standardized_data) 
 
# View the standardized data 
print(standardized_data) 
```

```{r}
#h)
#Guassian distribution - density plots for standardized data features
plots <- lapply(standardized_data, function(var) { 
  ggplot(data = standardized_data, aes(x = var)) +
    geom_density(fill = "#DC143C", color = "black") +
    theme_minimal() +
    labs(x = "Standardized Value", y = "Density")
})
plots
# Arrange the density plots in a grid
#grid.arrange(grobs = plots, ncol = 3)
```

```{r}
#h) Add a Diagnosis column to standardized_dataset 
standardized_data$Diagnosis <- patients_2$Diagnosis
standardized_data
```

```{r}
#######i) Create a logistic regression model (call it LRM1) using your best features. Describe your 
model. 
                           
#splitting the dataset into training and testing data (70:30)
#install.packages("caret")
library(caret)
set.seed(1234)
dindex <- createDataPartition(standardized_data$Diagnosis, p=0.7, list=FALSE)
train_data <- standardized_data[dindex,]
test_data <- standardized_data[-dindex,]
```

```{r}
#i)Model_1 using correlation plot

best_features <- c( "BMI", "Age", "Glucose","Pregnancies")
best_features_subset_data <- train_data[, c("Diagnosis", best_features)] 
#View(best_features_subset_data)

#LRM1 - using best features
LRM1 <- glm(Diagnosis ~ ., data = best_features_subset_data , family = binomial) 
 
# Print the summary of the model 
summary(LRM1)

```
```{r}
#######j & k) Create a classification report of your model. Describe your classification report (precision, recall, F1 score, and support).
                           
#install.packages("pROC")  # Install the pROC package
library(pROC)  # Load the pROC package
predictions <- predict(LRM1, newdata = test_data, type = "response")
predicted.classes <- ifelse(predictions> 0.5, 1, 0)
predicted.classes

library(caret)
confusion_matrix <- table(predicted.classes, test_data$Diagnosis)
confusion_matrix

precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
support <- rowSums(confusion_matrix)

# Print the classification report
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1-Score: ", f1_score, "\n")
cat("Support: ", support)
```

```{r}
#######l) Create the accuracy score of your model. Describe the accuracy score
                           
#accuracy of model1- LRM1 
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy: ", accuracy, "\n")
```

```{r}
#######m) Create another logistic regression model (call it LRM2). Use all the independent features this 
time (instead of your best performing features). 
                           
#LRM2 - using all features
LRM2 <- glm(Diagnosis ~ ., data = train_data , family = binomial) 
#print the summary of the model
summary(LRM2)
```

```{r}
#m)
predictions_model2 <- predict(LRM2, newdata = test_data, type = "response")
predicted.classes_model2 <- ifelse(predictions_model2> 0.5, 1, 0)
#predicted.classes_model2
```
```{r}
library(caret)
confusion_matrix_model2 <- table(predicted.classes_model2, test_data$Diagnosis)
confusion_matrix_model2
```
```{r}

precision <- confusion_matrix_model2[2, 2] / sum(confusion_matrix_model2[, 2])
recall <- confusion_matrix_model2[2, 2] / sum(confusion_matrix_model2[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
support <- rowSums(confusion_matrix_model2)

# Print the classification report
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1-Score: ", f1_score, "\n")
```

```{r}
#Accuracy of model2 - lRM2
accuracy <- sum(diag(confusion_matrix_model2)) / sum(confusion_matrix_model2)
cat("Accuracy: ", accuracy, "\n")
```

```{r}
#Model-3 
#LRM3 model based on features from literature review

features_model3 <- c( "BMI", "Pedigree", "Glucose", "Age")
model3_features_subset_data <- train_data[, c("Diagnosis", features_model3)] 

#LRM3
LRM3 <- glm(Diagnosis ~ ., data = model3_features_subset_data , family = binomial) 
 
# Print the summary of the model 
summary(LRM3)


predictions_model3 <- predict(LRM3, newdata = test_data, type = "response")
predicted.classes_model3 <- ifelse(predictions_model3> 0.5, 1, 0)
predicted.classes_model3

library(caret)
confusion_matrix_model3 <- table(predicted.classes_model3, test_data$Diagnosis)
confusion_matrix_model3

accuracy <- sum(diag(confusion_matrix_model3)) / sum(confusion_matrix_model3)
precision <- confusion_matrix_model3[2, 2] / sum(confusion_matrix_model3[, 2])
recall <- confusion_matrix_model3[2, 2] / sum(confusion_matrix_model3[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
support <- rowSums(confusion_matrix_model3)

# Print the classification report
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1-Score: ", f1_score, "\n")
cat("Support: ", support)
```

```{r}
#######n) Compare the two models (LRM1 and LRM2) based on the classification report and accuracy 
score. Which one is a better model? Why? 
                           
# Three models are developed namely LRM1,LRM2, LRM3 based on features selected from correlegram, all independent features and features based on literature review respectively.The model1 i.e, LRM1 is a better fit as it has highest accuracy compared to others.
```

```{r}
#######o) Examine the coefficients to understand the direction and significance of the relationship 
between the predictor variables and the outcome variable.
                           
#Coefficients are explained in the word doc
```

```{r}
#######p) Perform and interpret hypothesis tests that your model is significantly better. Explain the 
test statistic, degrees of freedom, and p-value associated with the hypothesis test.Wald tests for individual tests.
                           
confint(LRM3)
confint.default(LRM3)
#install.packages("lmtest")
#install.packages("aod")
library(lmtest)
library(zoo)
library(aod)
```

```{r}
#Wald test for BMI variable
wald.test(b = coef(LRM3), Sigma = vcov(LRM3), Terms = 2)
```

```{r}
#Wald test for Pedigree variable
wald.test(b = coef(LRM3), Sigma = vcov(LRM3), Terms = 3)
```

```{r}
#Wald test for Glucose variable
wald.test(b = coef(LRM3), Sigma = vcov(LRM3), Terms = 4)
```

```{r}
#Wald test for Age variable
wald.test(b = coef(LRM3), Sigma = vcov(LRM3), Terms = 5)
```

```{r}
#######q) After conducting the hypothesis tests, adjust the significance level for multiple comparisons 
using the Bonferroni correction. Use significance level = 0.05.
                           
# Obtain the p-values for your hypothesis tests
p_values <- c(2e-04 ,0.007, 0.0,9.1e-05)

# Determine the desired significance level
significance_level <- 0.05

# Calculate the Bonferroni-adjusted significance level
adjusted_significance_level <- significance_level / length(p_values)

# Compare individual p-values with the Bonferroni-adjusted significance level
significant_variables <- p_values <= adjusted_significance_level

# Print the significant variables
significant_variables

```


